<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Inderjit Singh Chahal</title><link>https://inderjit.in/post/</link><description>Recent content in Posts on Inderjit Singh Chahal</description><generator>Hugo -- gohugo.io</generator><managingEditor>inder@inderjit.in (Inderjit Singh Chahal)</managingEditor><webMaster>inder@inderjit.in (Inderjit Singh Chahal)</webMaster><lastBuildDate>Wed, 02 Jun 2021 10:00:01 +0530</lastBuildDate><atom:link href="https://inderjit.in/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Expanding Bernauli's Theorm to Real World - Amazon Reviews</title><link>https://inderjit.in/post/amazon_review/</link><pubDate>Wed, 02 Jun 2021 10:00:01 +0530</pubDate><author>inder@inderjit.in (Inderjit Singh Chahal)</author><guid>https://inderjit.in/post/amazon_review/</guid><description>Setting the objective In this blog we will try and explore the implementation of the Bernauli&amp;rsquo;s Theorm beyond the classical cases and try to get some useful information out of one of the implementation for a real world situation.
This blog will be part 1 of a two Part series where we explore bernauli&amp;rsquo;s theorm to get hold of probability of getting a positive experience after buying a product online given that there are number of sellers available and all of them have similar positive reviews based on different number of total reviews for the same product.</description></item><item><title>Pattern Exploiting Training for NLP</title><link>https://inderjit.in/post/pet_for_nlp/</link><pubDate>Thu, 08 Oct 2020 11:00:01 +0530</pubDate><author>inder@inderjit.in (Inderjit Singh Chahal)</author><guid>https://inderjit.in/post/pet_for_nlp/</guid><description>GPT-3 created quite a buzz when it was first launched a couple of months back for its ability to generate great good amount of text to create a more realistic feel, but it suffered from one particular shortcoming that its simply too big. There has been an increasing move towards creating more light weight implementations in NLP,CV and othe areas for machine learning and artificial intelligence, evident from the fact that most of the papers now along with accuracy also feature inference times for the respective model, a good sign of end use awareness in research.</description></item><item><title>How I finally understood Bayes Theorem</title><link>https://inderjit.in/post/2020-10-02-first-post/</link><pubDate>Fri, 02 Oct 2020 11:00:01 +0530</pubDate><author>inder@inderjit.in (Inderjit Singh Chahal)</author><guid>https://inderjit.in/post/2020-10-02-first-post/</guid><description>I came across Bayesian theorem a lot early in my education carrier, much like Thomas Bayes himself, I was not able to grasp the importance of this masterpiece. Additionally, much like most of the concepts at the time I just jumped over the understanding of the same and was mostly focused on solving some predefined pattern questions and never bothered to figure out how the theorem can actually be put to use in the real-world implementations.</description></item><item><title>Recommender System made easier with TFRS</title><link>https://inderjit.in/post/2020-09-29-first-post.md/</link><pubDate>Tue, 29 Sep 2020 11:00:01 +0530</pubDate><author>inder@inderjit.in (Inderjit Singh Chahal)</author><guid>https://inderjit.in/post/2020-09-29-first-post.md/</guid><description>Creating scalable deep learning-based recommendation systems from scratch is a significantly time-consuming task, especially so that we need to spend significant time on the non-productive elements involved in setting up the model for experiments. It is a given that we will need to perform levels of iterations on the model architecture to get optimal hyperparameters for a particular use case.
Google has now launched just the instrument required to reduce the non-productive component in that cycle.</description></item><item><title>Stochastic Process Part 1 - Introduction to Stochastic Processes</title><link>https://inderjit.in/post/stochastic-process-1/</link><pubDate>Thu, 17 Sep 2020 10:00:01 +0530</pubDate><author>inder@inderjit.in (Inderjit Singh Chahal)</author><guid>https://inderjit.in/post/stochastic-process-1/</guid><description>In this part 1, we will look at getting a better understanding of a stochastic process, in the next one we will look at some practical implementations to build on this understanding to solve some real-world problems.
What is a Stochastic Process Wikipedia defines a stochastic process as a mathematical object that is defined by a family of random variables.
A better way to look at it would be
It is a collection of random “events”, each happening with either fixed or varying probabilities, over a given time</description></item><item><title> Rediscovering math: How to win a game of shells</title><link>https://inderjit.in/post/2020-09-19-first-post/</link><pubDate>Fri, 04 Sep 2020 10:00:01 +0530</pubDate><author>inder@inderjit.in (Inderjit Singh Chahal)</author><guid>https://inderjit.in/post/2020-09-19-first-post/</guid><description>The time I moved out of my school, the image I had of mathematics was of something that you practice a lot to get good scores out of, you remember the “theorems” you see how they are derived if you want that extra edge, it was never something that could help solve real-world problems. I remember making arguments like where am I going to use these matrix multiplications in the real world?</description></item></channel></rss>